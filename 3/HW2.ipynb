{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tQ8_wMDEdIl"
      },
      "source": [
        "#Про корпус\n",
        "\n",
        "Мой корпус - собрание нескольких предложений, в которых содержатся трудные случаи для разметки PoS-тэгов. В основном туда входят случаи, где в разных контекстах одни и те же слова относятся к разным частям речи. Например:\n",
        "\n",
        "*Один (сущ.) в поле не воин.*\n",
        "\n",
        "*Диалекты слились в один (прил.) язык.*\n",
        "\n",
        "*В один (мест.) прекрасный день.*\n",
        "\n",
        "Примеры были взяты из справочников и грамматик. [Отсюда](https://pandia.ru/text/78/356/1392.php#:~:text=%D0%A1%D0%BB%D0%B5%D0%B4%D1%83%D0%B5%D1%82%20%D0%B8%D0%BC%D0%B5%D1%82%D1%8C%20%D0%B2%20%D0%B2%D0%B8%D0%B4%D1%83%2C%20%D1%87%D1%82%D0%BE,%D0%BE%D0%B4%D0%BD%D0%BE%D0%B9%20%D1%87%D0%B0%D1%81%D1%82%D0%B8%20%D1%80%D0%B5%D1%87%D0%B8%20%D0%B2%20%D0%B4%D1%80%D1%83%D0%B3%D1%83%D1%8E.&text=%D0%92%20%D1%81%D0%BE%D0%B2%D1%80%D0%B5%D0%BC%D0%B5%D0%BD%D0%BD%D0%BE%D0%BC%20%D1%80%D1%83%D1%81%D1%81%D0%BA%D0%BE%D0%BC%20%D1%8F%D0%B7%D1%8B%D0%BA%D0%B5%20%D0%BC%D0%BE%D0%B6%D0%BD%D0%BE%20%D0%B2%D1%8B%D0%B4%D0%B5%D0%BB%D0%B8%D1%82%D1%8C%20%D1%83%D0%B7%D1%83%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5%20%D0%B8%20%D0%BA%D0%BE%D0%BD%D1%82%D0%B5%D0%BA%D1%81%D1%82%D1%83%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5%20%D0%BE%D0%BC%D0%BE%D0%BD%D0%B8%D0%BC%D0%B8%D1%87%D0%BD%D1%8B%D0%B5%20%D1%87%D0%B0%D1%81%D1%82%D0%B8%20%D1%80%D0%B5%D1%87%D0%B8.) и [отсюда](https://videotutor-rusyaz.ru/uchenikam/teoriya/65-omonimiyaslov.html). А также некоторые я придумала сама, доволнив примеры словосочетаний контекстом для образования цельных предложений. Поэтому вопросов по поводу, каким PoS-тегом должен быть размечен каждый токен не возникло.\n",
        "\n",
        "#Про тегеры\n",
        "\n",
        "Для анализа я выбрала бибилиотеки spacy, stanza, natasha. Все они используют тег-сет UD, поэтому необходимости переводить теги в единую систему нет. Для разметки своего корпуса я пользовалась этим же тегсетом.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UFGFfG6HUfy"
      },
      "source": [
        "#Работа с тегами\n",
        "\n",
        "Сначала установим все необходимые бибилиотеки и напишем функции для разметки тегов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pitQHRWMiVwg"
      },
      "source": [
        "###Spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJQyFRnqcIbH"
      },
      "source": [
        "!pip install spacy==3.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu7lVssvcQka"
      },
      "source": [
        "!python -m spacy download ru_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt9zr5rAcSof"
      },
      "source": [
        "import spacy\n",
        "from spacy.lang.ru.examples import sentences \n",
        "\n",
        "nlp_spacy = spacy.load(\"ru_core_news_sm\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzdApZZpdHCJ"
      },
      "source": [
        "def pos_spacy(sent):\n",
        "    doc = nlp_spacy(sent)\n",
        "    return [(token.text, token.pos_) for token in doc if token.pos_ != 'SPACE']"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVdyxOGVdnWi"
      },
      "source": [
        "###Natasha"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9i30b01dcxa"
      },
      "source": [
        "!pip install natasha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0RbKSPXdmvJ"
      },
      "source": [
        "from natasha import (\n",
        "    Segmenter,\n",
        "    MorphVocab,\n",
        "    \n",
        "    NewsEmbedding,\n",
        "    NewsMorphTagger,\n",
        "    NewsSyntaxParser,\n",
        "    NewsNERTagger,\n",
        "    \n",
        "    PER,\n",
        "    NamesExtractor,\n",
        "\n",
        "    Doc\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tO6S2ihdyu0"
      },
      "source": [
        ">>> segmenter = Segmenter()\n",
        ">>> morph_vocab = MorphVocab()\n",
        "\n",
        ">>> emb = NewsEmbedding()\n",
        ">>> morph_tagger = NewsMorphTagger(emb)\n",
        ">>> syntax_parser = NewsSyntaxParser(emb)\n",
        ">>> ner_tagger = NewsNERTagger(emb)\n",
        "\n",
        ">>> names_extractor = NamesExtractor(morph_vocab)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B8F0G9SedLi"
      },
      "source": [
        "def pos_natasha(sent):\n",
        "    doc = Doc(sent)\n",
        "    doc.segment(segmenter)\n",
        "    doc.tag_morph(morph_tagger)\n",
        "    return [(x.text, x.pos) for x in doc.tokens]"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH2gzHjRggEh"
      },
      "source": [
        "###Stanza"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIPGpu2Qgeko"
      },
      "source": [
        "!pip install stanza"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK1kY0c2gh_T"
      },
      "source": [
        "import stanza\n",
        "stanza.download('ru') \n",
        "nlp = stanza.Pipeline('ru')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtsrTqdFhLuh"
      },
      "source": [
        "def pos_stanza(sent):\n",
        "    doc = nlp(sent)\n",
        "    return [(word.text, word.upos) for sent in doc.sentences for word in sent.words]"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4AQ5HD7yiOY"
      },
      "source": [
        "#Анализ\n",
        "\n",
        "Теперь прочитаем тексты корпуса: один чистый, другой размеченный вручную."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IACkVI9jXe5"
      },
      "source": [
        "import re"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R29FLWiTlfIq"
      },
      "source": [
        "with open('marked.txt', encoding='utf-8') as file:\n",
        "    PoS_true = []\n",
        "    for line in file:\n",
        "        PoS_true.extend(re.findall('([а-яА-ЯёЁ.?,-]+)_(\\w+)', line))"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAkBaVALjvZc"
      },
      "source": [
        "with open('corpora.txt', encoding='utf-8') as file:\n",
        "    text = file.read().replace('\\n', ' ')"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_DRMlteHt61"
      },
      "source": [
        "Разметим копус тегерами"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNRBwVhtlQDn"
      },
      "source": [
        "PoStags_spacy = pos_spacy(text)\n",
        "PoStags_natasha = pos_natasha(text)\n",
        "PoStags_stanza = pos_stanza(text)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUMqJQJEHyOX"
      },
      "source": [
        "Проверим на совпадение количества токенов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EUiEPTdmwBp",
        "outputId": "55883d16-a05b-4011-ebe3-828aa79688f6"
      },
      "source": [
        "len(PoStags_spacy), len(PoS_true), len(PoStags_natasha), len(PoStags_stanza)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(256, 254, 254, 254)"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV5CVgMzH2EP"
      },
      "source": [
        "У spacy получилось больше токенов, чем должно быть, потому что он делит слова с дефисом на несколько токенов. Для того, чтобы можно было сравнивать его работу с золотым стандартом, напишем функцию, которая объединяет такие токены в один."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlFZeBSqvksf"
      },
      "source": [
        "def correct_spacy(tokens):\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        if tokens[i][0] == '-':\n",
        "            new = tokens[:i-1]\n",
        "            correct_token = (tokens[i-1][0]+tokens[i][0]+tokens[i+1][0], tokens[i][1])\n",
        "            new.append(correct_token)\n",
        "            new.extend(tokens[i+2:])\n",
        "            tokens = new\n",
        "            i += 1\n",
        "        i += 1\n",
        "    return tokens"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgUROX4TxE_k"
      },
      "source": [
        "PoStags_spacy = correct_spacy(PoStags_spacy)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izurI2lLxdqr",
        "outputId": "6266a32f-6ddd-42fc-ec8e-8052fde83d82"
      },
      "source": [
        "len(PoStags_spacy), len(PoS_true)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(254, 254)"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LCd8ToeIKJY"
      },
      "source": [
        "Теперь все хорошо. Далее напишем функцию для измерения качества работы библиотек."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqnUDQHsx0G2"
      },
      "source": [
        "def count_accuracy(true, check):\n",
        "    metric = 0\n",
        "    for i in range(len(true)):\n",
        "        if check[i][1] == true[i][1]:\n",
        "            metric += 1\n",
        "    return metric / len(true)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bD64ues1yOu1",
        "outputId": "991bde34-7b6d-489e-e0de-41b044138f8f"
      },
      "source": [
        "print(f'spacy: {count_accuracy(PoS_true, PoStags_spacy)}')\n",
        "print(f'natasha: {count_accuracy(PoS_true, PoStags_natasha)}')\n",
        "print(f'stanza: {count_accuracy(PoS_true, PoStags_stanza)}')"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spacy: 0.8779527559055118\n",
            "natasha: 0.8622047244094488\n",
            "stanza: 0.8740157480314961\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZpJ93fzIejp"
      },
      "source": [
        "В целом, качество разнится не сильно. Проблемы возникают как раз при разметке омонимичных слов. Лучше всех справился spacy, хоть и с небольшим отрывом. \n",
        "\n",
        "(На самом деле, я не скажу, что мой корпус очень сбалансирован по случаям. Поэтому качество зависит еще и от того, каки случаев у меня больше. Например, если предложений, когда причастие переходит в прилагательное, могло оказаться больше, чем тех, где прилагательное - в существительное, то тегер, который лучше справляется с первой неоднозначностью, покажет качество выше, чем тот, который, наоборот, лучше справляется со вторым случаем омонимичности)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE6Ly9rzKLwg"
      },
      "source": [
        "#Chunker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKK9zuvT5UD7"
      },
      "source": [
        "def chunker(tokens):\n",
        "    adj_noun = []\n",
        "    verb_noun = []\n",
        "    verb_adp_noun = []\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        if tokens[i][1] == 'ADJ' and tokens[i+1][1] == 'NOUN':\n",
        "            adj_noun.append((tokens[i][0] + ' ' + tokens[i+1][0]))\n",
        "            i += 1\n",
        "        elif tokens[i][1] == 'VERB':\n",
        "            if tokens[i+1][1] == 'NOUN':\n",
        "                verb_noun.append((tokens[i][0] + ' ' + tokens[i+1][0]))\n",
        "                i += 1\n",
        "            elif tokens[i+1][1] == 'ADP' and tokens[i+2][1] == 'NOUN':\n",
        "                verb_adp_noun.append((tokens[i][0] + ' ' + tokens[i+1][0] + ' ' + tokens[i+2][0]))\n",
        "                i += 2\n",
        "        i += 1\n",
        "    return adj_noun, verb_noun, verb_adp_noun"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDyw6TxdMM0A",
        "outputId": "0b71e85c-a285-4f7c-aec6-30643a731192"
      },
      "source": [
        "a_n, v_n, v_a_n = chunker(PoStags_spacy)\n",
        "print(f'Прилагательное + существительное:\\n{a_n}\\n')\n",
        "print(f'Глагол + существительное:\\n{v_n}\\n')\n",
        "print(f'Глагол + предлог + существительное:\\n{v_a_n}')"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Прилагательное + существительное:\n",
            "['прекрасный день', 'южном тропике', 'темном лесе', 'красивые ели', 'ванную комнату', 'образованные люди']\n",
            "\n",
            "Глагол + существительное:\n",
            "['захватили штили', 'ели суп', 'кончилась ничьей', 'блестящий оратор', 'горящие дрова', 'горящие глаза', 'образованная взрывом', 'отдал сапоги', 'говори врастяжку']\n",
            "\n",
            "Глагол + предлог + существительное:\n",
            "['сидела в ванной', 'согласились на ничью', 'смотрели на иней', 'блестящий на солнце', 'собираться у камина', 'верится в правду']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jXBP05FM5j-"
      },
      "source": [
        "#Задание 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFPjL4WpA_qA"
      },
      "source": [
        "Если честно, я не поняла идею, чем может помочь добавление н-грам... \n",
        "\n",
        "Я добавила шаблоны:\n",
        "\n",
        "- прилагательное + существительное\n",
        "\n",
        "- наречие + прилагательное\n",
        "\n",
        "- наречие + прилагательное + существительное\n",
        "\n",
        "Эти н-грамы, вероятно, относятся к описательным, то есть передадут сентиментальную окраску комментариев.\n",
        "\n",
        "Сам код добавлен в конец тетрадки с предыдущей ДЗ.\n",
        "\n",
        "Качество упало. (опять повторюсь, я не поняла смысла этой идеи, как она могла улучшить качество?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD908W6sDf1H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
